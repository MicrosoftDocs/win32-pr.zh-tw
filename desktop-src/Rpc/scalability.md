---
title: 延展性
description: 延展性
ms.assetid: 39327621-b536-4494-9319-9e9d4f534123
keywords:
- 延展性
- 遠端程序呼叫 RPC、最佳作法、擴充性
ms.topic: article
ms.date: 05/31/2018
ms.openlocfilehash: 0728e35d9c9b27494014363c448be9965e39eea7
ms.sourcegitcommit: 2d531328b6ed82d4ad971a45a5131b430c5866f7
ms.translationtype: MT
ms.contentlocale: zh-TW
ms.lasthandoff: 09/16/2019
ms.locfileid: "103840620"
---
# <a name="scalability"></a><span data-ttu-id="dd055-105">延展性</span><span class="sxs-lookup"><span data-stu-id="dd055-105">Scalability</span></span>

<span data-ttu-id="dd055-106">這一期的擴充性通常是誤用的。</span><span class="sxs-lookup"><span data-stu-id="dd055-106">The term, scalability, is often misused.</span></span> <span data-ttu-id="dd055-107">在本節中，會提供雙重定義：</span><span class="sxs-lookup"><span data-stu-id="dd055-107">For this section, a dual definition is provided:</span></span>

-   <span data-ttu-id="dd055-108">擴充性是能夠在多處理器系統上充分利用可用的處理能力， (2、4、8、32或更多處理器) 。</span><span class="sxs-lookup"><span data-stu-id="dd055-108">Scalability is the ability to fully utilize available processing power on a multiprocessor system (2, 4, 8, 32, or more processors).</span></span>
-   <span data-ttu-id="dd055-109">擴充性是提供大量用戶端服務的能力。</span><span class="sxs-lookup"><span data-stu-id="dd055-109">Scalability is the ability to service a large number of clients.</span></span>

<span data-ttu-id="dd055-110">這兩個相關的定義通常稱為 *向上* 延展。</span><span class="sxs-lookup"><span data-stu-id="dd055-110">These two related definitions are commonly referred to as *scaling up*.</span></span> <span data-ttu-id="dd055-111">本主題結尾提供 *相應* 放大的秘訣。</span><span class="sxs-lookup"><span data-stu-id="dd055-111">The end of this topic provides tips about *scaling out*.</span></span>

<span data-ttu-id="dd055-112">這段討論僅著重于撰寫可擴充的伺服器，而不是可擴充的用戶端，因為可擴充的伺服器是更常見的需求。</span><span class="sxs-lookup"><span data-stu-id="dd055-112">This discussion focuses exclusively on writing scalable servers, not scalable clients, because scalable servers are more common requirements.</span></span> <span data-ttu-id="dd055-113">本節也可解決 RPC 和 RPC 伺服器內容中的擴充性。</span><span class="sxs-lookup"><span data-stu-id="dd055-113">This section also addresses scalability in the context of RPC and RPC servers only.</span></span> <span data-ttu-id="dd055-114">這裡不會討論擴充性的最佳做法，例如減少爭用、避免全域記憶體位置的頻繁快取遺漏，或避免共用錯誤。</span><span class="sxs-lookup"><span data-stu-id="dd055-114">Best practices for scalability, such as reducing contention, avoiding frequent cache misses on global memory locations, or avoiding false sharing, are not discussed here.</span></span>

## <a name="rpc-threading-model"></a><span data-ttu-id="dd055-115">RPC 執行緒模型</span><span class="sxs-lookup"><span data-stu-id="dd055-115">RPC Threading Model</span></span>

<span data-ttu-id="dd055-116">當伺服器收到 RPC 呼叫時，會在 RPC 提供的執行緒上呼叫 server 常式 (管理員常式) 。</span><span class="sxs-lookup"><span data-stu-id="dd055-116">When an RPC call is received by a server, the server routine (manager routine) is called on a thread supplied by RPC.</span></span> <span data-ttu-id="dd055-117">RPC 使用調適型執行緒集區，隨著工作負載波動而增加和減少。</span><span class="sxs-lookup"><span data-stu-id="dd055-117">RPC uses an adaptive thread pool that increases and decreases as workload fluctuates.</span></span> <span data-ttu-id="dd055-118">從 Windows 2000 開始，RPC 執行緒集區的核心是完成通訊埠。</span><span class="sxs-lookup"><span data-stu-id="dd055-118">Starting with Windows 2000, the core of the RPC thread pool is a completion port.</span></span> <span data-ttu-id="dd055-119">完成埠及其 RPC 的使用方式會針對零到低度的爭用伺服器常式進行調整。</span><span class="sxs-lookup"><span data-stu-id="dd055-119">The completion port and its usage by RPC are tuned for zero to low contention server routines.</span></span> <span data-ttu-id="dd055-120">這表示 RPC 執行緒集區會主動增加服務執行緒的數目（如果有封鎖執行緒）。</span><span class="sxs-lookup"><span data-stu-id="dd055-120">This means that the RPC thread pool aggressively increases the number of servicing threads if some become blocked.</span></span> <span data-ttu-id="dd055-121">它會在封鎖很罕見的了推測: 上運作，而且如果執行緒遭到封鎖，這就是可快速解決的暫時性狀況。</span><span class="sxs-lookup"><span data-stu-id="dd055-121">It operates on the presumption that blocking is rare, and if a thread gets blocked, this is a temporary condition that is quickly resolved.</span></span> <span data-ttu-id="dd055-122">這種方法可提高低爭用伺服器的效率。</span><span class="sxs-lookup"><span data-stu-id="dd055-122">This approach enables efficiency for low contention servers.</span></span> <span data-ttu-id="dd055-123">例如，在透過高速系統區域網路存取的八處理器550MHz 伺服器上運作的 void 呼叫 RPC 伺服器 (SAN) 會提供超過200個遠端用戶端每秒30000次的 void 呼叫。</span><span class="sxs-lookup"><span data-stu-id="dd055-123">For example, a void call RPC server operating on an eight-processor 550MHz server accessed over a high speed system area network (SAN) serves over 30,000 void calls per second from over 200 remote clients.</span></span> <span data-ttu-id="dd055-124">這代表每小時108000000個以上的呼叫。</span><span class="sxs-lookup"><span data-stu-id="dd055-124">This represents more than 108 million calls per hour.</span></span>

<span data-ttu-id="dd055-125">結果就是主動執行緒集區實際上是以伺服器上的爭用很高的方式來進行。</span><span class="sxs-lookup"><span data-stu-id="dd055-125">The result is that the aggressive thread pool actually gets in the way when contention on the server is high.</span></span> <span data-ttu-id="dd055-126">為了說明，請想像用來從遠端存取檔案的繁重伺服器。</span><span class="sxs-lookup"><span data-stu-id="dd055-126">To illustrate, imagine a heavy-duty server used to remotely access files.</span></span> <span data-ttu-id="dd055-127">假設伺服器採用最簡單的方法：它只會在 RPC 叫用伺服器常式的執行緒上同步讀取/寫入檔案。</span><span class="sxs-lookup"><span data-stu-id="dd055-127">Assume the server adopts the most straightforward approach: it simply reads/writes the file synchronously on the thread on which that RPC invokes the server routine.</span></span> <span data-ttu-id="dd055-128">此外，假設我們有四個處理器伺服器可提供許多用戶端。</span><span class="sxs-lookup"><span data-stu-id="dd055-128">Also, assume we have a four-processor server serving many clients.</span></span>

<span data-ttu-id="dd055-129">伺服器會從五個執行緒開始 (這實際上不同，但有五個執行緒用於簡化) 。</span><span class="sxs-lookup"><span data-stu-id="dd055-129">The server will start with five threads (this actually varies, but five threads is used for simplicity).</span></span> <span data-ttu-id="dd055-130">一旦 RPC 挑選第一個 RPC 呼叫，就會將呼叫分派給 server 常式，而伺服器常式會發出 i/o。</span><span class="sxs-lookup"><span data-stu-id="dd055-130">Once RPC picks up the first RPC call, it dispatches the call to the server routine, and the server routine issues the I/O.</span></span> <span data-ttu-id="dd055-131">它不常遺失檔案快取，然後封鎖等候結果。</span><span class="sxs-lookup"><span data-stu-id="dd055-131">Infrequently, it misses the file cache and then blocks waiting for the result.</span></span> <span data-ttu-id="dd055-132">一旦封鎖，第五個執行緒便會被釋放以收取要求，而第六個執行緒會建立為熱待命。</span><span class="sxs-lookup"><span data-stu-id="dd055-132">As soon as it blocks, the fifth thread is released to pick up a request, and a sixth thread is created as a hot standby.</span></span> <span data-ttu-id="dd055-133">假設每10個 i/o 作業遺漏了快取，而且會封鎖100毫秒 (任意時間值) ，並假設四個處理器伺服器每秒提供20000個呼叫 (5000 每個處理器) 的呼叫，則簡化的模型會預測每個處理器會產生大約50的執行緒。</span><span class="sxs-lookup"><span data-stu-id="dd055-133">Assuming each tenth I/O operation misses the cache and will block for 100 milliseconds (an arbitrary time value), and assuming the four-processor server serves about 20,000 calls per second (5,000 calls per processor), a simplistic modeling would predict that each processor will spawn approximately 50 threads.</span></span> <span data-ttu-id="dd055-134">這會假設將封鎖每隔2毫秒的呼叫，並在100毫秒後再次釋放第一個執行緒，讓集區在大約200個執行緒 (50 每個處理器) 。</span><span class="sxs-lookup"><span data-stu-id="dd055-134">This assumes a call that will block comes every 2 milliseconds, and after 100 milliseconds the first thread is freed again so the pool will stabilize at about 200 threads (50 per processor).</span></span>

<span data-ttu-id="dd055-135">實際的行為較為複雜，因為大量執行緒會造成額外的內容切換，而使伺服器變慢，而且也會降低建立新執行緒的速率，但基本概念很清楚。</span><span class="sxs-lookup"><span data-stu-id="dd055-135">The actual behavior is more complicated, as the high number of threads will cause extra context switches which slow the server, and also slow the rate of creation of new threads, but the basic idea is clear.</span></span> <span data-ttu-id="dd055-136">當伺服器上的執行緒開始封鎖並等候 (是 i/o 或資源) 的存取權時，執行緒的數目會很快就會出現。</span><span class="sxs-lookup"><span data-stu-id="dd055-136">The number of threads goes up quickly as threads on the server start blocking and waiting for something (be it an I/O, or access to a resource).</span></span>

<span data-ttu-id="dd055-137">RPC 和閘道連入要求的完成埠將嘗試維持伺服器中可用的 RPC 執行緒數目，使其等於電腦上的處理器數目。</span><span class="sxs-lookup"><span data-stu-id="dd055-137">RPC and the completion port that gates incoming requests will try to maintain the number of usable RPC threads in the server to be equal to the number of processors on the machine.</span></span> <span data-ttu-id="dd055-138">這表示在四個處理器的伺服器上，一旦執行緒回到 RPC 之後，如果有四個或更多可用的 RPC 執行緒，就不允許第五個執行緒挑選新的要求，而是在其中一個目前可用的執行緒區塊時，會進入熱待命狀態。</span><span class="sxs-lookup"><span data-stu-id="dd055-138">This means that on a four-processor server, once a thread returns to RPC, if there are four or more usable RPC threads, the fifth thread is not allowed to pick up a new request, and instead will sit in a hot standby state in case one of the currently usable threads blocks.</span></span> <span data-ttu-id="dd055-139">如果第五個執行緒的等候時間夠長，但沒有可使用的 RPC 執行緒數目低於處理器數目，就會釋出，也就是執行緒集區將會減少。</span><span class="sxs-lookup"><span data-stu-id="dd055-139">If the fifth thread waits long enough as a hot standby without the number of usable RPC threads dropping below the number of processors, it will be released, that is, the thread pool will decrease.</span></span>

<span data-ttu-id="dd055-140">想像有許多執行緒的伺服器。</span><span class="sxs-lookup"><span data-stu-id="dd055-140">Imagine a server with many threads.</span></span> <span data-ttu-id="dd055-141">如先前所述，RPC 伺服器最後會有許多執行緒，但只有執行緒經常封鎖。</span><span class="sxs-lookup"><span data-stu-id="dd055-141">As previously explained, an RPC server ends up with many threads, but only if the threads block often.</span></span> <span data-ttu-id="dd055-142">線上程經常封鎖的伺服器上，會立即從熱待命清單取出傳回至 RPC 的執行緒，因為所有目前可用的執行緒都會封鎖，而且會獲得處理要求。</span><span class="sxs-lookup"><span data-stu-id="dd055-142">On a server where threads often block, a thread that returns to RPC is soon taken out of the hot standby list, because all currently usable threads block, and is given a request to process.</span></span> <span data-ttu-id="dd055-143">當執行緒封鎖時，核心中的執行緒發送器會將內容切換到另一個執行緒。</span><span class="sxs-lookup"><span data-stu-id="dd055-143">When a thread blocks, the thread dispatcher in the kernel switches context to another thread.</span></span> <span data-ttu-id="dd055-144">此內容切換本身會耗用 CPU 迴圈。</span><span class="sxs-lookup"><span data-stu-id="dd055-144">This context switch by itself consumes CPU cycles.</span></span> <span data-ttu-id="dd055-145">下一個執行緒將會執行不同的程式碼、存取不同的資料結構，而且會有不同的堆疊，這表示 (L1 和 L2 快取的記憶體快取命中率) 將會較低，而導致執行速度變慢。</span><span class="sxs-lookup"><span data-stu-id="dd055-145">The next thread will be executing different code, accessing different data structures, and will have a different stack, which means the memory cache hit rate (the L1 and L2 caches) will be much lower, resulting in slower execution.</span></span> <span data-ttu-id="dd055-146">許多執行的執行緒都會同時增加現有資源的爭用，例如堆積、伺服器程式碼中的重要區段等等。</span><span class="sxs-lookup"><span data-stu-id="dd055-146">The numerous threads executing simultaneously increases contention for existing resources, such as heap, critical sections in the server code, and so on.</span></span> <span data-ttu-id="dd055-147">這會進一步將爭用增加為資源形式的群組。</span><span class="sxs-lookup"><span data-stu-id="dd055-147">This further increases contention as convoys on resources form.</span></span> <span data-ttu-id="dd055-148">如果記憶體不足，由大量和不斷成長的執行緒所行使的記憶體壓力會導致分頁錯誤，這會進一步增加執行緒封鎖的速率，並造成更多的執行緒建立。</span><span class="sxs-lookup"><span data-stu-id="dd055-148">If memory is low, the memory pressure exerted by the large and growing number of threads will cause page faults, which further increase the rate at which the threads block, and cause even more threads to be created.</span></span> <span data-ttu-id="dd055-149">根據它封鎖的頻率，以及可用的實體記憶體數量，伺服器可能會以較低的效能層級穩定，並以高內容交換器速率穩定，也可能會降低只重複存取硬碟和內容切換的時間點，而不會執行任何實際工作。</span><span class="sxs-lookup"><span data-stu-id="dd055-149">Depending on how often it blocks and how much physical memory is available, the server may either stabilize at some lower level of performance with a high context switch rate, or it may deteriorate to the point where it is only repeatedly accessing the hard disk and context switching without performing any actual work.</span></span> <span data-ttu-id="dd055-150">當然，這種情況並不會顯示在輕量工作負載下，但繁重的工作負載很快就會將問題帶入介面中。</span><span class="sxs-lookup"><span data-stu-id="dd055-150">This situation will not show under light workload, of course, but a heavy workload quickly brings the problem to the surface.</span></span>

<span data-ttu-id="dd055-151">如何防止這種情況？</span><span class="sxs-lookup"><span data-stu-id="dd055-151">How can this be prevented?</span></span> <span data-ttu-id="dd055-152">如果執行緒預期會封鎖、將呼叫宣告為非同步，並且在要求進入伺服器常式之後，將其佇列到使用 i/o 系統和/或 RPC 非同步功能的背景工作執行緒集區。</span><span class="sxs-lookup"><span data-stu-id="dd055-152">If threads are expected to block, declare calls as asynchronous, and once the request enters the server routine, queue it to a pool of worker threads that use the asynchronous capabilities of the I/O system and/or RPC.</span></span> <span data-ttu-id="dd055-153">如果伺服器正在進行 RPC 呼叫，請將它們設為非同步，並確定佇列的成長太大。</span><span class="sxs-lookup"><span data-stu-id="dd055-153">If the server is in turn making RPC calls make those asynchronous, and make sure the queue does not grow too large.</span></span> <span data-ttu-id="dd055-154">如果伺服器常式正在執行檔案 i/o，請使用非同步檔案 i/o 來將多個要求排入 i/o 系統的佇列，而且只有幾個執行緒將它們排入佇列，然後挑選結果。</span><span class="sxs-lookup"><span data-stu-id="dd055-154">If the server routine is performing file I/O, use asynchronous file I/O to queue multiple requests to the I/O system and have only a few threads queue them and pick up the results.</span></span> <span data-ttu-id="dd055-155">如果伺服器常式正在進行網路 i/o，請使用系統的非同步功能來發出要求，並以非同步方式挑選回復，並盡可能使用最少的執行緒。</span><span class="sxs-lookup"><span data-stu-id="dd055-155">If the server routine is doing network I/O, again, use the asynchronous capabilities of the system to issue the requests and pick up the replies asynchronously, and use as few threads as possible.</span></span> <span data-ttu-id="dd055-156">當 i/o 完成，或執行伺服器的 RPC 呼叫完成時，請完成傳遞要求的非同步 RPC 呼叫。</span><span class="sxs-lookup"><span data-stu-id="dd055-156">When the I/O is done, or the RPC call the server made is complete, complete the asynchronous RPC call that delivered the request.</span></span> <span data-ttu-id="dd055-157">如此一來，伺服器就能以最少的執行緒執行，進而提高效能以及伺服器可以服務的用戶端數目。</span><span class="sxs-lookup"><span data-stu-id="dd055-157">This will enable the server to run with as few threads as possible, which increases the performance and the number of clients a server can service.</span></span>

## <a name="scale-out"></a><span data-ttu-id="dd055-158">擴增</span><span class="sxs-lookup"><span data-stu-id="dd055-158">Scale Out</span></span>

<span data-ttu-id="dd055-159">您可以將 RPC 設定為使用網路負載平衡 (NLB) 如果設定 NLB，讓指定用戶端位址的所有要求都移至相同的伺服器。</span><span class="sxs-lookup"><span data-stu-id="dd055-159">RPC can be configured to work with Network Load Balancing (NLB) if NLB is configured such that all requests from a given client address go to the same server.</span></span> <span data-ttu-id="dd055-160">由於每個 RPC 用戶端會開啟連接集區 (如需詳細資訊，請參閱 [RPC 和網路](rpc-and-the-network.md)) ，來自指定用戶端集區的所有連接都必須在相同的伺服器電腦上。</span><span class="sxs-lookup"><span data-stu-id="dd055-160">Because each RPC client opens a connection pool (for more information, see [RPC and the Network](rpc-and-the-network.md)), it is essential that all connections from the pool of the given client end up on the same server computer.</span></span> <span data-ttu-id="dd055-161">只要符合這種情況，您就可以將 NLB 叢集設定為一部大型 RPC 伺服器，而且可能會有絕佳的擴充性。</span><span class="sxs-lookup"><span data-stu-id="dd055-161">As long as this condition is met, an NLB cluster can be configured to function as one large RPC server with potentially excellent scalability.</span></span>

 

 




